{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "sentence-transformers/all-MiniLM-L6-v2 does not appear to have a file named pytorch_model.bin but there is a file for TensorFlow weights. Use `from_tf=True` to load this model from those weights.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Inicialize o extrator com o caminho do PDF\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m extractor \u001b[38;5;241m=\u001b[39m \u001b[43mPDFRAGExtractor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetenv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPDF_COMPLETO\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msentence-transformers/all-MiniLM-L6-v2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeneration_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgoogle/flan-t5-base\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# ou \"cpu\"\u001b[39;49;00m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Fazer queries\u001b[39;00m\n\u001b[0;32m     13\u001b[0m resultado \u001b[38;5;241m=\u001b[39m extractor\u001b[38;5;241m.\u001b[39mprocess_pdf(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtraia todos os campos do registro 0220\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32me:\\Carla Reis\\Projects\\Notebooks\\efd-ia\\efd_utils\\pdf_rag_extractor.py:34\u001b[0m, in \u001b[0;36mPDFRAGExtractor.__init__\u001b[1;34m(self, pdf_path, embedding_model, generation_model, device)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m device\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Inicializa o modelo de embeddings com configurações otimizadas\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings \u001b[38;5;241m=\u001b[39m \u001b[43mHuggingFaceEmbeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdevice\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencode_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Inicializa o modelo de geração com configurações otimizadas\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m     42\u001b[0m     generation_model,\n\u001b[0;32m     43\u001b[0m     low_cpu_mem_usage\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     44\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages\\langchain_huggingface\\embeddings\\huggingface.py:59\u001b[0m, in \u001b[0;36mHuggingFaceEmbeddings.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     55\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import sentence_transformers python package. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     56\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install it with `pip install sentence-transformers`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     57\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client \u001b[38;5;241m=\u001b[39m sentence_transformers\u001b[38;5;241m.\u001b[39mSentenceTransformer(\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name, cache_folder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_folder, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_kwargs\n\u001b[0;32m     61\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:309\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[1;34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[0m\n\u001b[0;32m    300\u001b[0m         model_name_or_path \u001b[38;5;241m=\u001b[39m __MODEL_HUB_ORGANIZATION__ \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m model_name_or_path\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sentence_transformer_model(\n\u001b[0;32m    303\u001b[0m     model_name_or_path,\n\u001b[0;32m    304\u001b[0m     token,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    307\u001b[0m     local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m    308\u001b[0m ):\n\u001b[1;32m--> 309\u001b[0m     modules, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_sbert_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    321\u001b[0m     modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_auto_model(\n\u001b[0;32m    322\u001b[0m         model_name_or_path,\n\u001b[0;32m    323\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    330\u001b[0m         config_kwargs\u001b[38;5;241m=\u001b[39mconfig_kwargs,\n\u001b[0;32m    331\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:1808\u001b[0m, in \u001b[0;36mSentenceTransformer._load_sbert_model\u001b[1;34m(self, model_name_or_path, token, cache_folder, revision, trust_remote_code, local_files_only, model_kwargs, tokenizer_kwargs, config_kwargs)\u001b[0m\n\u001b[0;32m   1805\u001b[0m \u001b[38;5;66;03m# Try to initialize the module with a lot of kwargs, but only if the module supports them\u001b[39;00m\n\u001b[0;32m   1806\u001b[0m \u001b[38;5;66;03m# Otherwise we fall back to the load method\u001b[39;00m\n\u001b[0;32m   1807\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1808\u001b[0m     module \u001b[38;5;241m=\u001b[39m module_class(model_name_or_path, cache_dir\u001b[38;5;241m=\u001b[39mcache_folder, backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1809\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   1810\u001b[0m     module \u001b[38;5;241m=\u001b[39m module_class\u001b[38;5;241m.\u001b[39mload(model_name_or_path)\n",
      "File \u001b[1;32mc:\\Users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py:81\u001b[0m, in \u001b[0;36mTransformer.__init__\u001b[1;34m(self, model_name_or_path, max_seq_length, model_args, tokenizer_args, config_args, cache_dir, do_lower_case, tokenizer_name_or_path, backend)\u001b[0m\n\u001b[0;32m     78\u001b[0m     config_args \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     80\u001b[0m config, is_peft_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_config(model_name_or_path, cache_dir, backend, config_args)\n\u001b[1;32m---> 81\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_model(model_name_or_path, config, cache_dir, backend, is_peft_model, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_args)\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_seq_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_max_length\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m tokenizer_args:\n\u001b[0;32m     84\u001b[0m     tokenizer_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_max_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m max_seq_length\n",
      "File \u001b[1;32mc:\\Users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py:181\u001b[0m, in \u001b[0;36mTransformer._load_model\u001b[1;34m(self, model_name_or_path, config, cache_dir, backend, is_peft_model, **model_args)\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_mt5_model(model_name_or_path, config, cache_dir, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_args)\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 181\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_model \u001b[38;5;241m=\u001b[39m AutoModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    182\u001b[0m         model_name_or_path, config\u001b[38;5;241m=\u001b[39mconfig, cache_dir\u001b[38;5;241m=\u001b[39mcache_dir, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_args\n\u001b[0;32m    183\u001b[0m     )\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_peft_model:\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_peft_model(model_name_or_path, config, cache_dir, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39madapter_only_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:571\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mconfig_class \u001b[38;5;241m==\u001b[39m config\u001b[38;5;241m.\u001b[39msub_configs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    570\u001b[0m         config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_text_config()\n\u001b[1;32m--> 571\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    572\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    573\u001b[0m     )\n\u001b[0;32m    574\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    575\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    576\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    577\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages\\transformers\\modeling_utils.py:279\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    281\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[1;32mc:\\Users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages\\transformers\\modeling_utils.py:4260\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   4250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   4251\u001b[0m     gguf_file\n\u001b[0;32m   4252\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   4253\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m ((\u001b[38;5;28misinstance\u001b[39m(device_map, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map)\n\u001b[0;32m   4254\u001b[0m ):\n\u001b[0;32m   4255\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   4256\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOne or more modules is configured to be mapped to disk. Disk offload is not supported for models \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4257\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloaded from GGUF files.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4258\u001b[0m     )\n\u001b[1;32m-> 4260\u001b[0m checkpoint_files, sharded_metadata \u001b[38;5;241m=\u001b[39m \u001b[43m_get_resolved_checkpoint_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4262\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvariant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgguf_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgguf_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4267\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4273\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4276\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4278\u001b[0m is_sharded \u001b[38;5;241m=\u001b[39m sharded_metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   4279\u001b[0m is_quantized \u001b[38;5;241m=\u001b[39m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages\\transformers\\modeling_utils.py:1080\u001b[0m, in \u001b[0;36m_get_resolved_checkpoint_files\u001b[1;34m(pretrained_model_name_or_path, subfolder, variant, gguf_file, from_tf, from_flax, use_safetensors, cache_dir, force_download, proxies, local_files_only, token, user_agent, revision, commit_hash)\u001b[0m\n\u001b[0;32m   1072\u001b[0m has_file_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1073\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrevision\u001b[39m\u001b[38;5;124m\"\u001b[39m: revision,\n\u001b[0;32m   1074\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproxies\u001b[39m\u001b[38;5;124m\"\u001b[39m: proxies,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1077\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal_files_only\u001b[39m\u001b[38;5;124m\"\u001b[39m: local_files_only,\n\u001b[0;32m   1078\u001b[0m }\n\u001b[0;32m   1079\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_file(pretrained_model_name_or_path, TF2_WEIGHTS_NAME, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhas_file_kwargs):\n\u001b[1;32m-> 1080\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m   1081\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not appear to have a file named\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1082\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_add_variant(WEIGHTS_NAME,\u001b[38;5;250m \u001b[39mvariant)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m but there is a file for TensorFlow weights.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1083\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Use `from_tf=True` to load this model from those weights.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1084\u001b[0m     )\n\u001b[0;32m   1085\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m has_file(pretrained_model_name_or_path, FLAX_WEIGHTS_NAME, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhas_file_kwargs):\n\u001b[0;32m   1086\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m   1087\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not appear to have a file named\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1088\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_add_variant(WEIGHTS_NAME,\u001b[38;5;250m \u001b[39mvariant)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m but there is a file for Flax weights. Use\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1089\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `from_flax=True` to load this model from those weights.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1090\u001b[0m     )\n",
      "\u001b[1;31mOSError\u001b[0m: sentence-transformers/all-MiniLM-L6-v2 does not appear to have a file named pytorch_model.bin but there is a file for TensorFlow weights. Use `from_tf=True` to load this model from those weights."
     ]
    }
   ],
   "source": [
    "from efd_utils.pdf_rag_extractor import PDFRAGExtractor\n",
    "import os\n",
    "\n",
    "# Inicialize o extrator com o caminho do PDF\n",
    "extractor = PDFRAGExtractor(\n",
    "    pdf_path=os.getenv('PDF_COMPLETO'),\n",
    "    embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    generation_model=\"google/flan-t5-base\",\n",
    "    device=\"cuda\"  # ou \"cpu\"\n",
    "    )\n",
    "\n",
    "# Fazer queries\n",
    "resultado = extractor.process_pdf(\"Extraia todos os campos do registro 0220\")\n",
    "print(json.dumps(resultado, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: tensorflow 2.19.0\n",
      "Uninstalling tensorflow-2.19.0:\n",
      "  Successfully uninstalled tensorflow-2.19.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages\\~ensorflow'.\n",
      "You can safely remove it manually.\n",
      "WARNING: Skipping tensorflow-text as it is not installed.\n",
      "WARNING: Skipping tensorflow-hub as it is not installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.7.0-cp310-cp310-win_amd64.whl.metadata (29 kB)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.22.0-cp310-cp310-win_amd64.whl.metadata (6.3 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.7.0-cp310-cp310-win_amd64.whl.metadata (6.7 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting typing-extensions>=4.10.0 (from torch)\n",
      "  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Using cached fsspec-2025.3.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting numpy (from torchvision)\n",
      "  Using cached numpy-2.2.5-cp310-cp310-win_amd64.whl.metadata (60 kB)\n",
      "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision)\n",
      "  Using cached pillow-11.2.1-cp310-cp310-win_amd64.whl.metadata (9.1 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Using cached MarkupSafe-3.0.2-cp310-cp310-win_amd64.whl.metadata (4.1 kB)\n",
      "Downloading torch-2.7.0-cp310-cp310-win_amd64.whl (212.5 MB)\n",
      "   ---------------------------------------- 0.0/212.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 8.9/212.5 MB 61.8 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 26.2/212.5 MB 69.2 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 37.7/212.5 MB 63.1 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 43.8/212.5 MB 55.7 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 44.0/212.5 MB 49.2 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 55.8/212.5 MB 45.0 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 62.1/212.5 MB 43.1 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 68.4/212.5 MB 41.2 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 74.7/212.5 MB 40.0 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 81.3/212.5 MB 39.0 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 83.6/212.5 MB 36.8 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 84.1/212.5 MB 33.8 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 84.4/212.5 MB 32.4 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 84.9/212.5 MB 29.8 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 85.5/212.5 MB 27.7 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 86.0/212.5 MB 25.9 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 86.2/212.5 MB 25.1 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 86.8/212.5 MB 23.2 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 87.3/212.5 MB 21.8 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 87.6/212.5 MB 21.1 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 88.1/212.5 MB 19.9 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 88.1/212.5 MB 19.9 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 88.1/212.5 MB 19.9 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 88.6/212.5 MB 17.6 MB/s eta 0:00:08\n",
      "   ------------------ -------------------- 101.4/212.5 MB 19.0 MB/s eta 0:00:06\n",
      "   -------------------- ------------------ 110.6/212.5 MB 20.0 MB/s eta 0:00:06\n",
      "   --------------------- ----------------- 117.4/212.5 MB 20.4 MB/s eta 0:00:05\n",
      "   ---------------------- ---------------- 124.8/212.5 MB 20.9 MB/s eta 0:00:05\n",
      "   ------------------------ -------------- 132.1/212.5 MB 21.4 MB/s eta 0:00:04\n",
      "   ------------------------ -------------- 134.5/212.5 MB 21.1 MB/s eta 0:00:04\n",
      "   ------------------------ -------------- 134.7/212.5 MB 20.7 MB/s eta 0:00:04\n",
      "   ------------------------ -------------- 134.7/212.5 MB 20.7 MB/s eta 0:00:04\n",
      "   ------------------------ -------------- 135.0/212.5 MB 19.4 MB/s eta 0:00:05\n",
      "   ------------------------ -------------- 135.5/212.5 MB 18.9 MB/s eta 0:00:05\n",
      "   ------------------------ -------------- 136.1/212.5 MB 18.3 MB/s eta 0:00:05\n",
      "   ------------------------- ------------- 136.3/212.5 MB 18.0 MB/s eta 0:00:05\n",
      "   ------------------------- ------------- 136.8/212.5 MB 17.3 MB/s eta 0:00:05\n",
      "   ------------------------- ------------- 137.1/212.5 MB 17.2 MB/s eta 0:00:05\n",
      "   ------------------------- ------------- 137.6/212.5 MB 16.7 MB/s eta 0:00:05\n",
      "   ------------------------- ------------- 138.1/212.5 MB 16.3 MB/s eta 0:00:05\n",
      "   ------------------------- ------------- 138.4/212.5 MB 16.0 MB/s eta 0:00:05\n",
      "   ------------------------- ------------- 138.9/212.5 MB 15.5 MB/s eta 0:00:05\n",
      "   ------------------------- ------------- 139.2/212.5 MB 15.2 MB/s eta 0:00:05\n",
      "   ------------------------- ------------- 139.5/212.5 MB 14.9 MB/s eta 0:00:05\n",
      "   ------------------------- ------------- 140.0/212.5 MB 14.7 MB/s eta 0:00:05\n",
      "   ------------------------- ------------- 140.5/212.5 MB 14.3 MB/s eta 0:00:06\n",
      "   ------------------------- ------------- 140.5/212.5 MB 14.3 MB/s eta 0:00:06\n",
      "   ------------------------- ------------- 140.5/212.5 MB 14.3 MB/s eta 0:00:06\n",
      "   ------------------------- ------------- 140.8/212.5 MB 13.5 MB/s eta 0:00:06\n",
      "   ------------------------- ------------- 141.0/212.5 MB 13.4 MB/s eta 0:00:06\n",
      "   ---------------------------- ---------- 156.0/212.5 MB 14.3 MB/s eta 0:00:04\n",
      "   ----------------------------- --------- 161.7/212.5 MB 14.5 MB/s eta 0:00:04\n",
      "   ------------------------------ -------- 167.0/212.5 MB 14.7 MB/s eta 0:00:04\n",
      "   ------------------------------- ------- 170.7/212.5 MB 14.7 MB/s eta 0:00:03\n",
      "   ------------------------------- ------- 173.0/212.5 MB 14.8 MB/s eta 0:00:03\n",
      "   ------------------------------- ------- 173.0/212.5 MB 14.8 MB/s eta 0:00:03\n",
      "   ------------------------------- ------- 173.0/212.5 MB 14.8 MB/s eta 0:00:03\n",
      "   ------------------------------- ------- 173.5/212.5 MB 14.0 MB/s eta 0:00:03\n",
      "   ------------------------------- ------- 174.1/212.5 MB 13.8 MB/s eta 0:00:03\n",
      "   -------------------------------- ------ 177.7/212.5 MB 13.8 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 191.9/212.5 MB 14.7 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 192.4/212.5 MB 14.5 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 192.7/212.5 MB 14.3 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 206.0/212.5 MB 15.0 MB/s eta 0:00:01\n",
      "   --------------------------------------  210.8/212.5 MB 15.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  212.3/212.5 MB 15.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  212.3/212.5 MB 15.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  212.3/212.5 MB 15.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  212.3/212.5 MB 15.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  212.3/212.5 MB 15.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  212.3/212.5 MB 15.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  212.3/212.5 MB 15.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  212.3/212.5 MB 15.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  212.3/212.5 MB 15.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  212.3/212.5 MB 15.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  212.3/212.5 MB 15.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  212.3/212.5 MB 15.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  212.3/212.5 MB 15.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  212.3/212.5 MB 15.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  212.3/212.5 MB 15.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  212.3/212.5 MB 15.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  212.3/212.5 MB 15.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  212.3/212.5 MB 15.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  212.3/212.5 MB 15.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  212.3/212.5 MB 15.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  212.3/212.5 MB 15.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  212.3/212.5 MB 15.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  212.3/212.5 MB 15.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  212.3/212.5 MB 15.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  212.3/212.5 MB 15.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  212.3/212.5 MB 15.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  212.3/212.5 MB 15.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  212.3/212.5 MB 15.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  212.3/212.5 MB 15.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  212.3/212.5 MB 15.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  212.3/212.5 MB 15.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  212.3/212.5 MB 15.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  212.3/212.5 MB 15.1 MB/s eta 0:00:01\n",
      "   --------------------------------------- 212.5/212.5 MB 10.2 MB/s eta 0:00:00\n",
      "Downloading torchvision-0.22.0-cp310-cp310-win_amd64.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.7/1.7 MB 11.6 MB/s eta 0:00:00\n",
      "Downloading torchaudio-2.7.0-cp310-cp310-win_amd64.whl (2.5 MB)\n",
      "   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.5/2.5 MB 14.2 MB/s eta 0:00:00\n",
      "Using cached pillow-11.2.1-cp310-cp310-win_amd64.whl (2.7 MB)\n",
      "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   ------------------------------- -------- 5.0/6.3 MB 23.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.3/6.3 MB 16.8 MB/s eta 0:00:00\n",
      "Downloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
      "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Using cached fsspec-2025.3.2-py3-none-any.whl (194 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Downloading numpy-2.2.5-cp310-cp310-win_amd64.whl (12.9 MB)\n",
      "   ---------------------------------------- 0.0/12.9 MB ? eta -:--:--\n",
      "   --------------- ------------------------ 5.0/12.9 MB 25.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 10.5/12.9 MB 25.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.8/12.9 MB 25.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.8/12.9 MB 25.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.9/12.9 MB 15.0 MB/s eta 0:00:00\n",
      "Using cached MarkupSafe-3.0.2-cp310-cp310-win_amd64.whl (15 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, typing-extensions, sympy, pillow, numpy, networkx, MarkupSafe, fsspec, filelock, jinja2, torch, torchvision, torchaudio\n",
      "  Attempting uninstall: mpmath\n",
      "    Found existing installation: mpmath 1.3.0\n",
      "    Uninstalling mpmath-1.3.0:\n",
      "      Successfully uninstalled mpmath-1.3.0\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.13.2\n",
      "    Uninstalling typing_extensions-4.13.2:\n",
      "      Successfully uninstalled typing_extensions-4.13.2\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.1\n",
      "    Uninstalling sympy-1.13.1:\n",
      "      Successfully uninstalled sympy-1.13.1\n",
      "  Attempting uninstall: pillow\n",
      "    Found existing installation: pillow 10.4.0\n",
      "    Uninstalling pillow-10.4.0:\n",
      "      Successfully uninstalled pillow-10.4.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.1.3\n",
      "    Uninstalling numpy-2.1.3:\n",
      "      Successfully uninstalled numpy-2.1.3\n",
      "  Attempting uninstall: networkx\n",
      "    Found existing installation: networkx 3.4.2\n",
      "    Uninstalling networkx-3.4.2:\n",
      "      Successfully uninstalled networkx-3.4.2\n",
      "  Attempting uninstall: MarkupSafe\n",
      "    Found existing installation: MarkupSafe 3.0.2\n",
      "    Uninstalling MarkupSafe-3.0.2:\n",
      "      Successfully uninstalled MarkupSafe-3.0.2\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.2\n",
      "    Uninstalling fsspec-2025.3.2:\n",
      "      Successfully uninstalled fsspec-2025.3.2\n",
      "  Attempting uninstall: filelock\n",
      "    Found existing installation: filelock 3.18.0\n",
      "    Uninstalling filelock-3.18.0:\n",
      "      Successfully uninstalled filelock-3.18.0\n",
      "  Attempting uninstall: jinja2\n",
      "    Found existing installation: Jinja2 3.1.6\n",
      "    Uninstalling Jinja2-3.1.6:\n",
      "      Successfully uninstalled Jinja2-3.1.6\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.5.1\n",
      "    Uninstalling torch-2.5.1:\n",
      "      Successfully uninstalled torch-2.5.1\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.20.1\n",
      "    Uninstalling torchvision-0.20.1:\n",
      "      Successfully uninstalled torchvision-0.20.1\n",
      "  Attempting uninstall: torchaudio\n",
      "    Found existing installation: torchaudio 2.5.1\n",
      "    Uninstalling torchaudio-2.5.1:\n",
      "      Successfully uninstalled torchaudio-2.5.1\n",
      "Successfully installed MarkupSafe-3.0.2 filelock-3.18.0 fsspec-2025.3.2 jinja2-3.1.6 mpmath-1.3.0 networkx-3.4.2 numpy-2.2.5 pillow-11.2.1 sympy-1.14.0 torch-2.7.0 torchaudio-2.7.0 torchvision-0.22.0 typing-extensions-4.13.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages\\~umpy.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages\\~umpy'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-experimental 0.0.65 requires langchain-community<0.3.0,>=0.2.16, but you have langchain-community 0.3.22 which is incompatible.\n",
      "langchain-experimental 0.0.65 requires langchain-core<0.3.0,>=0.2.38, but you have langchain-core 0.3.56 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages (4.1.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages (4.51.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages (from sentence-transformers) (2.7.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages (from sentence-transformers) (1.15.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages (from sentence-transformers) (0.30.2)\n",
      "Requirement already satisfied: Pillow in c:\\users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages (from sentence-transformers) (11.2.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages (from sentence-transformers) (4.13.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages (from transformers) (2.2.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'rm' n�o � reconhecido como um comando interno\n",
      "ou externo, um programa oper�vel ou um arquivo em lotes.\n"
     ]
    }
   ],
   "source": [
    "# Execute no Jupyter Notebook ANTES de rodar o código principal\n",
    "!pip uninstall -y tensorflow tensorflow-text tensorflow-hub\n",
    "!pip install --force-reinstall torch torchvision torchaudio\n",
    "!pip install -U sentence-transformers transformers\n",
    "!rm -rf ~/.cache/huggingface/hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.1.0\n"
     ]
    }
   ],
   "source": [
    "import sentence_transformers\n",
    "print(sentence_transformers.__version__)  # Deve ser ≥ 2.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "sentence-transformers/all-MiniLM-L6-v2 does not appear to have a file named pytorch_model.bin but there is a file for TensorFlow weights. Use `from_tf=True` to load this model from those weights.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[1;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msentence-transformers/all-MiniLM-L6-v2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mteste de funcionamento\u001b[39m\u001b[38;5;124m\"\u001b[39m, convert_to_tensor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "File \u001b[1;32mc:\\Users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:309\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[1;34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[0m\n\u001b[0;32m    300\u001b[0m         model_name_or_path \u001b[38;5;241m=\u001b[39m __MODEL_HUB_ORGANIZATION__ \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m model_name_or_path\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sentence_transformer_model(\n\u001b[0;32m    303\u001b[0m     model_name_or_path,\n\u001b[0;32m    304\u001b[0m     token,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    307\u001b[0m     local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m    308\u001b[0m ):\n\u001b[1;32m--> 309\u001b[0m     modules, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_sbert_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    321\u001b[0m     modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_auto_model(\n\u001b[0;32m    322\u001b[0m         model_name_or_path,\n\u001b[0;32m    323\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    330\u001b[0m         config_kwargs\u001b[38;5;241m=\u001b[39mconfig_kwargs,\n\u001b[0;32m    331\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:1808\u001b[0m, in \u001b[0;36mSentenceTransformer._load_sbert_model\u001b[1;34m(self, model_name_or_path, token, cache_folder, revision, trust_remote_code, local_files_only, model_kwargs, tokenizer_kwargs, config_kwargs)\u001b[0m\n\u001b[0;32m   1805\u001b[0m \u001b[38;5;66;03m# Try to initialize the module with a lot of kwargs, but only if the module supports them\u001b[39;00m\n\u001b[0;32m   1806\u001b[0m \u001b[38;5;66;03m# Otherwise we fall back to the load method\u001b[39;00m\n\u001b[0;32m   1807\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1808\u001b[0m     module \u001b[38;5;241m=\u001b[39m module_class(model_name_or_path, cache_dir\u001b[38;5;241m=\u001b[39mcache_folder, backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1809\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   1810\u001b[0m     module \u001b[38;5;241m=\u001b[39m module_class\u001b[38;5;241m.\u001b[39mload(model_name_or_path)\n",
      "File \u001b[1;32mc:\\Users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py:81\u001b[0m, in \u001b[0;36mTransformer.__init__\u001b[1;34m(self, model_name_or_path, max_seq_length, model_args, tokenizer_args, config_args, cache_dir, do_lower_case, tokenizer_name_or_path, backend)\u001b[0m\n\u001b[0;32m     78\u001b[0m     config_args \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     80\u001b[0m config, is_peft_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_config(model_name_or_path, cache_dir, backend, config_args)\n\u001b[1;32m---> 81\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_model(model_name_or_path, config, cache_dir, backend, is_peft_model, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_args)\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_seq_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_max_length\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m tokenizer_args:\n\u001b[0;32m     84\u001b[0m     tokenizer_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_max_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m max_seq_length\n",
      "File \u001b[1;32mc:\\Users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py:181\u001b[0m, in \u001b[0;36mTransformer._load_model\u001b[1;34m(self, model_name_or_path, config, cache_dir, backend, is_peft_model, **model_args)\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_mt5_model(model_name_or_path, config, cache_dir, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_args)\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 181\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_model \u001b[38;5;241m=\u001b[39m AutoModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    182\u001b[0m         model_name_or_path, config\u001b[38;5;241m=\u001b[39mconfig, cache_dir\u001b[38;5;241m=\u001b[39mcache_dir, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_args\n\u001b[0;32m    183\u001b[0m     )\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_peft_model:\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_peft_model(model_name_or_path, config, cache_dir, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39madapter_only_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:571\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mconfig_class \u001b[38;5;241m==\u001b[39m config\u001b[38;5;241m.\u001b[39msub_configs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    570\u001b[0m         config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_text_config()\n\u001b[1;32m--> 571\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    572\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    573\u001b[0m     )\n\u001b[0;32m    574\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    575\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    576\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    577\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages\\transformers\\modeling_utils.py:279\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    281\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[1;32mc:\\Users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages\\transformers\\modeling_utils.py:4260\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   4250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   4251\u001b[0m     gguf_file\n\u001b[0;32m   4252\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   4253\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m ((\u001b[38;5;28misinstance\u001b[39m(device_map, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map)\n\u001b[0;32m   4254\u001b[0m ):\n\u001b[0;32m   4255\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   4256\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOne or more modules is configured to be mapped to disk. Disk offload is not supported for models \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4257\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloaded from GGUF files.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4258\u001b[0m     )\n\u001b[1;32m-> 4260\u001b[0m checkpoint_files, sharded_metadata \u001b[38;5;241m=\u001b[39m \u001b[43m_get_resolved_checkpoint_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4262\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvariant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgguf_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgguf_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4267\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4273\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4276\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4278\u001b[0m is_sharded \u001b[38;5;241m=\u001b[39m sharded_metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   4279\u001b[0m is_quantized \u001b[38;5;241m=\u001b[39m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages\\transformers\\modeling_utils.py:1080\u001b[0m, in \u001b[0;36m_get_resolved_checkpoint_files\u001b[1;34m(pretrained_model_name_or_path, subfolder, variant, gguf_file, from_tf, from_flax, use_safetensors, cache_dir, force_download, proxies, local_files_only, token, user_agent, revision, commit_hash)\u001b[0m\n\u001b[0;32m   1072\u001b[0m has_file_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1073\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrevision\u001b[39m\u001b[38;5;124m\"\u001b[39m: revision,\n\u001b[0;32m   1074\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproxies\u001b[39m\u001b[38;5;124m\"\u001b[39m: proxies,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1077\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal_files_only\u001b[39m\u001b[38;5;124m\"\u001b[39m: local_files_only,\n\u001b[0;32m   1078\u001b[0m }\n\u001b[0;32m   1079\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_file(pretrained_model_name_or_path, TF2_WEIGHTS_NAME, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhas_file_kwargs):\n\u001b[1;32m-> 1080\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m   1081\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not appear to have a file named\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1082\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_add_variant(WEIGHTS_NAME,\u001b[38;5;250m \u001b[39mvariant)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m but there is a file for TensorFlow weights.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1083\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Use `from_tf=True` to load this model from those weights.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1084\u001b[0m     )\n\u001b[0;32m   1085\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m has_file(pretrained_model_name_or_path, FLAX_WEIGHTS_NAME, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhas_file_kwargs):\n\u001b[0;32m   1086\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m   1087\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not appear to have a file named\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1088\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_add_variant(WEIGHTS_NAME,\u001b[38;5;250m \u001b[39mvariant)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m but there is a file for Flax weights. Use\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1089\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `from_flax=True` to load this model from those weights.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1090\u001b[0m     )\n",
      "\u001b[1;31mOSError\u001b[0m: sentence-transformers/all-MiniLM-L6-v2 does not appear to have a file named pytorch_model.bin but there is a file for TensorFlow weights. Use `from_tf=True` to load this model from those weights."
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "print(model.encode(\"teste de funcionamento\", convert_to_tensor=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name e:\\Carla Reis\\Projects\\Notebooks\\efd-ia\\models\\all-MiniLM-L6-v2. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: (384,)\n"
     ]
    }
   ],
   "source": [
    "# Teste independente do modelo de embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(os.path.abspath(\"models/all-MiniLM-L6-v2\"))\n",
    "embedding = model.encode(\"teste de funcionamento\")\n",
    "print(f\"Embedding shape: {embedding.shape}\")  # Deve retornar (384,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "HFValidationError",
     "evalue": "Repo id must be in the form 'repo_name' or 'namespace/repo_name': 'google/flan-t5-base/tree/main'. Use `repo_type` argument if needed.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhuggingface_hub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m snapshot_download\n\u001b[1;32m----> 3\u001b[0m \u001b[43msnapshot_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgoogle/flan-t5-base/tree/main\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_patterns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m*.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m*.h5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizer/*\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels/flan-t5-small\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:106\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m arg_name, arg_value \u001b[38;5;129;01min\u001b[39;00m chain(\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28mzip\u001b[39m(signature\u001b[38;5;241m.\u001b[39mparameters, args),  \u001b[38;5;66;03m# Args values\u001b[39;00m\n\u001b[0;32m    103\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mitems(),  \u001b[38;5;66;03m# Kwargs values\u001b[39;00m\n\u001b[0;32m    104\u001b[0m ):\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m--> 106\u001b[0m         \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    109\u001b[0m         has_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:154\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[1;34m(repo_id)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must be a string, not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(repo_id)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m repo_id\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[0;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must be in the form \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnamespace/repo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Use `repo_type` argument if needed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    157\u001b[0m     )\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[0;32m    161\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must use alphanumeric chars or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m are\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    162\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m forbidden, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m cannot start or end the name, max length is 96:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    163\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    164\u001b[0m     )\n",
      "\u001b[1;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': 'google/flan-t5-base/tree/main'. Use `repo_type` argument if needed."
     ]
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=\"google/flan-t5-base/tree/main\",\n",
    "    allow_patterns=[\"*.json\", \"*.h5\", \"tokenizer/*\"],\n",
    "    local_dir=\"models/flan-t5-small\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name e:\\Carla Reis\\Projects\\Notebooks\\efd-ia\\models\\all-MiniLM-L6-v2. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b53dcd20e1014110888aa302c8c648a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e09661e602f24613955299008ee1ebc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "428dbb5d46ae4337abf22434c88ea8d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dfb3ec4afcf462286fe189b553aa1e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OSError",
     "evalue": "google/flan-t5-small does not appear to have a file named pytorch_model.bin but there is a file for TensorFlow weights. Use `from_tf=True` to load this model from those weights.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 11\u001b[0m\n\u001b[0;32m      7\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTOKENIZERS_PARALLELISM\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfalse\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Modelo ajustado para evitar conflitos\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m extractor \u001b[38;5;241m=\u001b[39m \u001b[43mPDFRAGExtractor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetenv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPDF_COMPLETO\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43membedding_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabspath\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels/all-MiniLM-L6-v2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgoogle/flan-t5-small\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Modelo mais leve\u001b[39;49;00m\n\u001b[0;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Processamento com tratamento de erro\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32me:\\Carla Reis\\Projects\\Notebooks\\efd-ia\\efd_utils\\pdf_rag_extractor.py:47\u001b[0m, in \u001b[0;36mPDFRAGExtractor.__init__\u001b[1;34m(self, pdf_path, embedding_model, generation_model, device)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m     42\u001b[0m     generation_model,\n\u001b[0;32m     43\u001b[0m     use_fast\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     44\u001b[0m )\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Carregamento seguro do modelo de geração\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForSeq2SeqLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeneration_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Carrega os pesos do TensorFlow\u001b[39;49;00m\n\u001b[0;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use float32 para CPU\u001b[39;49;00m\n\u001b[0;32m     52\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Cria o pipeline de geração com configurações otimizadas\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_pipeline \u001b[38;5;241m=\u001b[39m pipeline(\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext2text-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     58\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     63\u001b[0m     device\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     64\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:571\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mconfig_class \u001b[38;5;241m==\u001b[39m config\u001b[38;5;241m.\u001b[39msub_configs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    570\u001b[0m         config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_text_config()\n\u001b[1;32m--> 571\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    572\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    573\u001b[0m     )\n\u001b[0;32m    574\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    575\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    576\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    577\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages\\transformers\\modeling_utils.py:279\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    281\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[1;32mc:\\Users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages\\transformers\\modeling_utils.py:4260\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   4250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   4251\u001b[0m     gguf_file\n\u001b[0;32m   4252\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   4253\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m ((\u001b[38;5;28misinstance\u001b[39m(device_map, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map)\n\u001b[0;32m   4254\u001b[0m ):\n\u001b[0;32m   4255\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   4256\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOne or more modules is configured to be mapped to disk. Disk offload is not supported for models \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4257\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloaded from GGUF files.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4258\u001b[0m     )\n\u001b[1;32m-> 4260\u001b[0m checkpoint_files, sharded_metadata \u001b[38;5;241m=\u001b[39m \u001b[43m_get_resolved_checkpoint_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4262\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvariant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgguf_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgguf_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4267\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4273\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4276\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4278\u001b[0m is_sharded \u001b[38;5;241m=\u001b[39m sharded_metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   4279\u001b[0m is_quantized \u001b[38;5;241m=\u001b[39m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages\\transformers\\modeling_utils.py:1080\u001b[0m, in \u001b[0;36m_get_resolved_checkpoint_files\u001b[1;34m(pretrained_model_name_or_path, subfolder, variant, gguf_file, from_tf, from_flax, use_safetensors, cache_dir, force_download, proxies, local_files_only, token, user_agent, revision, commit_hash)\u001b[0m\n\u001b[0;32m   1072\u001b[0m has_file_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1073\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrevision\u001b[39m\u001b[38;5;124m\"\u001b[39m: revision,\n\u001b[0;32m   1074\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproxies\u001b[39m\u001b[38;5;124m\"\u001b[39m: proxies,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1077\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal_files_only\u001b[39m\u001b[38;5;124m\"\u001b[39m: local_files_only,\n\u001b[0;32m   1078\u001b[0m }\n\u001b[0;32m   1079\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_file(pretrained_model_name_or_path, TF2_WEIGHTS_NAME, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhas_file_kwargs):\n\u001b[1;32m-> 1080\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m   1081\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not appear to have a file named\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1082\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_add_variant(WEIGHTS_NAME,\u001b[38;5;250m \u001b[39mvariant)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m but there is a file for TensorFlow weights.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1083\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Use `from_tf=True` to load this model from those weights.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1084\u001b[0m     )\n\u001b[0;32m   1085\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m has_file(pretrained_model_name_or_path, FLAX_WEIGHTS_NAME, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhas_file_kwargs):\n\u001b[0;32m   1086\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m   1087\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not appear to have a file named\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1088\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_add_variant(WEIGHTS_NAME,\u001b[38;5;250m \u001b[39mvariant)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m but there is a file for Flax weights. Use\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1089\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `from_flax=True` to load this model from those weights.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1090\u001b[0m     )\n",
      "\u001b[1;31mOSError\u001b[0m: google/flan-t5-small does not appear to have a file named pytorch_model.bin but there is a file for TensorFlow weights. Use `from_tf=True` to load this model from those weights."
     ]
    }
   ],
   "source": [
    "from efd_utils.pdf_rag_extractor import PDFRAGExtractor\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Configurações para Notebook\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "    \n",
    "    \n",
    "# Modelo ajustado para evitar conflitos\n",
    "extractor = PDFRAGExtractor(\n",
    "        pdf_path=os.getenv('PDF_COMPLETO'),\n",
    "        embedding_model=os.path.abspath(\"models/all-MiniLM-L6-v2\"),\n",
    "        generation_model=\"google/flan-t5-small\",  # Modelo mais leve\n",
    "        device=\"cpu\"\n",
    "    )\n",
    "    \n",
    "# Processamento com tratamento de erro\n",
    "try:\n",
    "        query = \"Extraia todos os campos e suas descrições do registro 0220\"\n",
    "        result = extractor.process_pdf(query)\n",
    "        \n",
    "        output_path = os.path.join('..', 'data', 'structured', 'extracted_data.json')\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(result, f, ensure_ascii=False, indent=4)\n",
    "            \n",
    "        print(f\"Dados extraídos salvos em: {output_path}\")\n",
    "        \n",
    "except Exception as e:\n",
    "        print(f\"Erro durante o processamento: {str(e)}\")\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Collecting torch==2.3.0+cpu\n",
      "  Using cached https://download.pytorch.org/whl/cpu/torch-2.3.0%2Bcpu-cp310-cp310-win_amd64.whl (161.7 MB)\n",
      "Requirement already satisfied: filelock in c:\\users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages (from torch==2.3.0+cpu) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages (from torch==2.3.0+cpu) (4.13.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages (from torch==2.3.0+cpu) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages (from torch==2.3.0+cpu) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages (from torch==2.3.0+cpu) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages (from torch==2.3.0+cpu) (2025.3.2)\n",
      "Collecting mkl<=2021.4.0,>=2021.1.1 (from torch==2.3.0+cpu)\n",
      "  Downloading mkl-2021.4.0-py2.py3-none-win_amd64.whl.metadata (1.4 kB)\n",
      "Collecting intel-openmp==2021.* (from mkl<=2021.4.0,>=2021.1.1->torch==2.3.0+cpu)\n",
      "  Downloading intel_openmp-2021.4.0-py2.py3-none-win_amd64.whl.metadata (1.2 kB)\n",
      "Collecting tbb==2021.* (from mkl<=2021.4.0,>=2021.1.1->torch==2.3.0+cpu)\n",
      "  Using cached tbb-2021.13.1-py3-none-win_amd64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages (from jinja2->torch==2.3.0+cpu) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages (from sympy->torch==2.3.0+cpu) (1.3.0)\n",
      "Downloading mkl-2021.4.0-py2.py3-none-win_amd64.whl (228.5 MB)\n",
      "   ---------------------------------------- 0.0/228.5 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 11.8/228.5 MB 73.9 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 25.2/228.5 MB 93.6 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 48.8/228.5 MB 86.3 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 73.9/228.5 MB 94.3 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 95.4/228.5 MB 98.2 MB/s eta 0:00:02\n",
      "   ----------------- --------------------- 103.5/228.5 MB 84.7 MB/s eta 0:00:02\n",
      "   ---------------------- ---------------- 129.2/228.5 MB 89.8 MB/s eta 0:00:02\n",
      "   -------------------------- ------------ 154.4/228.5 MB 93.9 MB/s eta 0:00:01\n",
      "   --------------------------- ----------- 161.5/228.5 MB 86.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------ 189.0/228.5 MB 90.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------ 192.9/228.5 MB 86.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 193.5/228.5 MB 77.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 193.7/228.5 MB 73.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 194.2/228.5 MB 67.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 194.8/228.5 MB 62.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 195.0/228.5 MB 60.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 195.6/228.5 MB 57.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 195.8/228.5 MB 54.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 196.3/228.5 MB 51.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 196.9/228.5 MB 48.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 197.4/228.5 MB 46.0 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 197.9/228.5 MB 43.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 198.2/228.5 MB 42.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 198.4/228.5 MB 40.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 199.0/228.5 MB 38.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 199.5/228.5 MB 37.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 199.8/228.5 MB 36.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 200.3/228.5 MB 35.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 200.8/228.5 MB 33.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 201.1/228.5 MB 33.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 201.3/228.5 MB 32.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 201.6/228.5 MB 31.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 202.1/228.5 MB 29.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 202.6/228.5 MB 28.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 202.9/228.5 MB 28.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 203.2/228.5 MB 27.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 203.7/228.5 MB 26.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 204.2/228.5 MB 26.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 204.2/228.5 MB 26.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 204.5/228.5 MB 24.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  223.1/228.5 MB 26.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.3/228.5 MB 26.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.3/228.5 MB 26.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.3/228.5 MB 26.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.3/228.5 MB 26.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.3/228.5 MB 26.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.3/228.5 MB 26.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.3/228.5 MB 26.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.3/228.5 MB 26.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.3/228.5 MB 26.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.3/228.5 MB 26.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.3/228.5 MB 26.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.3/228.5 MB 26.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.3/228.5 MB 26.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.3/228.5 MB 26.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.3/228.5 MB 26.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.3/228.5 MB 26.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.3/228.5 MB 26.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.3/228.5 MB 26.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.3/228.5 MB 26.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.3/228.5 MB 26.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.3/228.5 MB 26.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.3/228.5 MB 26.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.3/228.5 MB 26.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.3/228.5 MB 26.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.3/228.5 MB 26.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.3/228.5 MB 26.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.3/228.5 MB 26.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.3/228.5 MB 26.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.3/228.5 MB 26.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.3/228.5 MB 26.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.3/228.5 MB 26.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.3/228.5 MB 26.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.3/228.5 MB 26.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.3/228.5 MB 26.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.3/228.5 MB 26.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.3/228.5 MB 26.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.3/228.5 MB 26.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.3/228.5 MB 26.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.3/228.5 MB 26.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.3/228.5 MB 26.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.3/228.5 MB 26.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.3/228.5 MB 26.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.3/228.5 MB 26.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.3/228.5 MB 26.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.3/228.5 MB 26.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.3/228.5 MB 26.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.3/228.5 MB 26.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.3/228.5 MB 26.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.3/228.5 MB 26.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.3/228.5 MB 26.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.3/228.5 MB 26.4 MB/s eta 0:00:01\n",
      "   --------------------------------------- 228.5/228.5 MB 11.6 MB/s eta 0:00:00\n",
      "Downloading intel_openmp-2021.4.0-py2.py3-none-win_amd64.whl (3.5 MB)\n",
      "   ---------------------------------------- 0.0/3.5 MB ? eta -:--:--\n",
      "   -------------------------------------- - 3.4/3.5 MB 68.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.5/3.5 MB 11.0 MB/s eta 0:00:00\n",
      "Using cached tbb-2021.13.1-py3-none-win_amd64.whl (286 kB)\n",
      "Installing collected packages: tbb, intel-openmp, mkl, torch\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.7.0\n",
      "    Uninstalling torch-2.7.0:\n",
      "      Successfully uninstalled torch-2.7.0\n",
      "Successfully installed intel-openmp-2021.4.0 mkl-2021.4.0 tbb-2021.13.1 torch-2.3.0+cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages\\~orch'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.7.0 requires torch==2.7.0, but you have torch 2.3.0+cpu which is incompatible.\n",
      "torchvision 0.22.0 requires torch==2.7.0, but you have torch 2.3.0+cpu which is incompatible.\n",
      "ERROR: Invalid requirement: '#': Expected package name at the start of dependency specifier\n",
      "    #\n",
      "    ^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.41.0\n",
      "  Downloading transformers-4.41.0-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting sentence-transformers==3.0.0\n",
      "  Downloading sentence_transformers-3.0.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages (from transformers==4.41.0) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in c:\\users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages (from transformers==4.41.0) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages (from transformers==4.41.0) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages (from transformers==4.41.0) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages (from transformers==4.41.0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages (from transformers==4.41.0) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages (from transformers==4.41.0) (2.32.3)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers==4.41.0)\n",
      "  Downloading tokenizers-0.19.1-cp310-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages (from transformers==4.41.0) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages (from transformers==4.41.0) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages (from sentence-transformers==3.0.0) (2.3.0+cpu)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages (from sentence-transformers==3.0.0) (1.6.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages (from sentence-transformers==3.0.0) (1.15.2)\n",
      "Requirement already satisfied: Pillow in c:\\users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages (from sentence-transformers==3.0.0) (11.2.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.41.0) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.41.0) (4.13.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages (from torch>=1.11.0->sentence-transformers==3.0.0) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages (from torch>=1.11.0->sentence-transformers==3.0.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages (from torch>=1.11.0->sentence-transformers==3.0.0) (3.1.6)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages (from torch>=1.11.0->sentence-transformers==3.0.0) (2021.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages (from tqdm>=4.27->transformers==4.41.0) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages (from requests->transformers==4.41.0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages (from requests->transformers==4.41.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages (from requests->transformers==4.41.0) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages (from requests->transformers==4.41.0) (2025.1.31)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages (from scikit-learn->sentence-transformers==3.0.0) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages (from scikit-learn->sentence-transformers==3.0.0) (3.6.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.11.0->sentence-transformers==3.0.0) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.11.0->sentence-transformers==3.0.0) (2021.13.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers==3.0.0) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages (from sympy->torch>=1.11.0->sentence-transformers==3.0.0) (1.3.0)\n",
      "Downloading transformers-4.41.0-py3-none-any.whl (9.1 MB)\n",
      "   ---------------------------------------- 0.0/9.1 MB ? eta -:--:--\n",
      "   --------------------------- ------------ 6.3/9.1 MB 38.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  8.9/9.1 MB 42.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.1/9.1 MB 20.9 MB/s eta 0:00:00\n",
      "Downloading sentence_transformers-3.0.0-py3-none-any.whl (224 kB)\n",
      "Downloading tokenizers-0.19.1-cp310-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.2/2.2 MB 12.5 MB/s eta 0:00:00\n",
      "Installing collected packages: tokenizers, transformers, sentence-transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.21.1\n",
      "    Uninstalling tokenizers-0.21.1:\n",
      "      Successfully uninstalled tokenizers-0.21.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.51.3\n",
      "    Uninstalling transformers-4.51.3:\n",
      "      Successfully uninstalled transformers-4.51.3\n",
      "  Attempting uninstall: sentence-transformers\n",
      "    Found existing installation: sentence-transformers 4.1.0\n",
      "    Uninstalling sentence-transformers-4.1.0:\n",
      "      Successfully uninstalled sentence-transformers-4.1.0\n",
      "Successfully installed sentence-transformers-3.0.0 tokenizers-0.19.1 transformers-4.41.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\clr_c\\AppData\\Local\\Temp\\pip-uninstall-w8ddynv4'.\n",
      "  You can safely remove it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting huggingface-hub==0.23.3\n",
      "  Downloading huggingface_hub-0.23.3-py3-none-any.whl.metadata (12 kB)\n",
      "Downloading huggingface_hub-0.23.3-py3-none-any.whl (401 kB)\n",
      "Installing collected packages: huggingface-hub\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.30.2\n",
      "    Uninstalling huggingface-hub-0.30.2:\n",
      "      Successfully uninstalled huggingface-hub-0.30.2\n",
      "Successfully installed huggingface-hub-0.23.3\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==2.3.0+cpu -f https://download.pytorch.org/whl/torch_stable.html\n",
    "!pip install tensorflow==2.16.1  # Versão compatível\n",
    "!pip install transformers==4.41.0 sentence-transformers==3.0.0\n",
    "!pip install huggingface-hub==0.23.3 --no-deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree(os.path.expanduser('~/.cache/huggingface/hub'), ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages\\tf_keras\\src\\initializers\\initializers.py:121: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n",
      "c:\\Users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for shared.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      "c:\\Users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for lm_head.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for T5ForConditionalGeneration:\n\tsize mismatch for encoder.block.0.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.0.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.0.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.0.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([32, 12]).\n\tsize mismatch for encoder.block.0.layer.0.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.block.0.layer.1.DenseReluDense.wi_0.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for encoder.block.0.layer.1.DenseReluDense.wi_1.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for encoder.block.0.layer.1.DenseReluDense.wo.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for encoder.block.0.layer.1.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.block.1.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.1.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.1.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.1.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.1.layer.0.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.block.1.layer.1.DenseReluDense.wi_0.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for encoder.block.1.layer.1.DenseReluDense.wi_1.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for encoder.block.1.layer.1.DenseReluDense.wo.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for encoder.block.1.layer.1.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.block.2.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.2.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.2.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.2.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.2.layer.0.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.block.2.layer.1.DenseReluDense.wi_0.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for encoder.block.2.layer.1.DenseReluDense.wi_1.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for encoder.block.2.layer.1.DenseReluDense.wo.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for encoder.block.2.layer.1.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.block.3.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.3.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.3.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.3.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.3.layer.0.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.block.3.layer.1.DenseReluDense.wi_0.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for encoder.block.3.layer.1.DenseReluDense.wi_1.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for encoder.block.3.layer.1.DenseReluDense.wo.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for encoder.block.3.layer.1.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.block.4.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.4.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.4.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.4.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.4.layer.0.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.block.4.layer.1.DenseReluDense.wi_0.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for encoder.block.4.layer.1.DenseReluDense.wi_1.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for encoder.block.4.layer.1.DenseReluDense.wo.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for encoder.block.4.layer.1.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.block.5.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.5.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.5.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.5.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.5.layer.0.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.block.5.layer.1.DenseReluDense.wi_0.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for encoder.block.5.layer.1.DenseReluDense.wi_1.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for encoder.block.5.layer.1.DenseReluDense.wo.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for encoder.block.5.layer.1.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.block.6.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.6.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.6.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.6.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.6.layer.0.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.block.6.layer.1.DenseReluDense.wi_0.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for encoder.block.6.layer.1.DenseReluDense.wi_1.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for encoder.block.6.layer.1.DenseReluDense.wo.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for encoder.block.6.layer.1.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.block.7.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.7.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.7.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.7.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.7.layer.0.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.block.7.layer.1.DenseReluDense.wi_0.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for encoder.block.7.layer.1.DenseReluDense.wi_1.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for encoder.block.7.layer.1.DenseReluDense.wo.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for encoder.block.7.layer.1.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.block.8.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.8.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.8.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.8.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.8.layer.0.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.block.8.layer.1.DenseReluDense.wi_0.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for encoder.block.8.layer.1.DenseReluDense.wi_1.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for encoder.block.8.layer.1.DenseReluDense.wo.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for encoder.block.8.layer.1.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.block.9.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.9.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.9.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.9.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.9.layer.0.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.block.9.layer.1.DenseReluDense.wi_0.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for encoder.block.9.layer.1.DenseReluDense.wi_1.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for encoder.block.9.layer.1.DenseReluDense.wo.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for encoder.block.9.layer.1.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.block.10.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.10.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.10.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.10.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.10.layer.0.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.block.10.layer.1.DenseReluDense.wi_0.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for encoder.block.10.layer.1.DenseReluDense.wi_1.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for encoder.block.10.layer.1.DenseReluDense.wo.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for encoder.block.10.layer.1.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.block.11.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.11.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.11.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.11.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.11.layer.0.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.block.11.layer.1.DenseReluDense.wi_0.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for encoder.block.11.layer.1.DenseReluDense.wi_1.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for encoder.block.11.layer.1.DenseReluDense.wo.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for encoder.block.11.layer.1.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.final_layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for decoder.block.0.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.0.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.0.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.0.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([32, 12]).\n\tsize mismatch for decoder.block.0.layer.0.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for decoder.block.0.layer.1.EncDecAttention.q.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.0.layer.1.EncDecAttention.k.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.0.layer.1.EncDecAttention.v.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.0.layer.1.EncDecAttention.o.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.0.layer.1.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for decoder.block.0.layer.2.DenseReluDense.wi_0.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for decoder.block.0.layer.2.DenseReluDense.wi_1.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for decoder.block.0.layer.2.DenseReluDense.wo.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for decoder.block.0.layer.2.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for decoder.block.1.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.1.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.1.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.1.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.1.layer.0.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for decoder.block.1.layer.1.EncDecAttention.q.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.1.layer.1.EncDecAttention.k.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.1.layer.1.EncDecAttention.v.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.1.layer.1.EncDecAttention.o.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.1.layer.1.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for decoder.block.1.layer.2.DenseReluDense.wi_0.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for decoder.block.1.layer.2.DenseReluDense.wi_1.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for decoder.block.1.layer.2.DenseReluDense.wo.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for decoder.block.1.layer.2.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for decoder.block.2.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.2.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.2.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.2.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.2.layer.0.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for decoder.block.2.layer.1.EncDecAttention.q.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.2.layer.1.EncDecAttention.k.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.2.layer.1.EncDecAttention.v.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.2.layer.1.EncDecAttention.o.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.2.layer.1.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for decoder.block.2.layer.2.DenseReluDense.wi_0.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for decoder.block.2.layer.2.DenseReluDense.wi_1.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for decoder.block.2.layer.2.DenseReluDense.wo.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for decoder.block.2.layer.2.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for decoder.block.3.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.3.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.3.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.3.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.3.layer.0.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for decoder.block.3.layer.1.EncDecAttention.q.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.3.layer.1.EncDecAttention.k.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.3.layer.1.EncDecAttention.v.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.3.layer.1.EncDecAttention.o.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.3.layer.1.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for decoder.block.3.layer.2.DenseReluDense.wi_0.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for decoder.block.3.layer.2.DenseReluDense.wi_1.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for decoder.block.3.layer.2.DenseReluDense.wo.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for decoder.block.3.layer.2.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for decoder.block.4.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.4.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.4.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.4.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.4.layer.0.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for decoder.block.4.layer.1.EncDecAttention.q.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.4.layer.1.EncDecAttention.k.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.4.layer.1.EncDecAttention.v.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.4.layer.1.EncDecAttention.o.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.4.layer.1.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for decoder.block.4.layer.2.DenseReluDense.wi_0.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for decoder.block.4.layer.2.DenseReluDense.wi_1.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for decoder.block.4.layer.2.DenseReluDense.wo.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for decoder.block.4.layer.2.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for decoder.block.5.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.5.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.5.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.5.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.5.layer.0.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for decoder.block.5.layer.1.EncDecAttention.q.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.5.layer.1.EncDecAttention.k.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.5.layer.1.EncDecAttention.v.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.5.layer.1.EncDecAttention.o.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.5.layer.1.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for decoder.block.5.layer.2.DenseReluDense.wi_0.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for decoder.block.5.layer.2.DenseReluDense.wi_1.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for decoder.block.5.layer.2.DenseReluDense.wo.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for decoder.block.5.layer.2.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for decoder.block.6.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.6.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.6.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.6.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.6.layer.0.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for decoder.block.6.layer.1.EncDecAttention.q.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.6.layer.1.EncDecAttention.k.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.6.layer.1.EncDecAttention.v.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.6.layer.1.EncDecAttention.o.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.6.layer.1.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for decoder.block.6.layer.2.DenseReluDense.wi_0.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for decoder.block.6.layer.2.DenseReluDense.wi_1.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for decoder.block.6.layer.2.DenseReluDense.wo.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for decoder.block.6.layer.2.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for decoder.block.7.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.7.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.7.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.7.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.7.layer.0.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for decoder.block.7.layer.1.EncDecAttention.q.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.7.layer.1.EncDecAttention.k.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.7.layer.1.EncDecAttention.v.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.7.layer.1.EncDecAttention.o.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.7.layer.1.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for decoder.block.7.layer.2.DenseReluDense.wi_0.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for decoder.block.7.layer.2.DenseReluDense.wi_1.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for decoder.block.7.layer.2.DenseReluDense.wo.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for decoder.block.7.layer.2.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for decoder.block.8.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.8.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.8.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.8.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.8.layer.0.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for decoder.block.8.layer.1.EncDecAttention.q.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.8.layer.1.EncDecAttention.k.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.8.layer.1.EncDecAttention.v.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.8.layer.1.EncDecAttention.o.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.8.layer.1.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for decoder.block.8.layer.2.DenseReluDense.wi_0.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for decoder.block.8.layer.2.DenseReluDense.wi_1.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for decoder.block.8.layer.2.DenseReluDense.wo.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for decoder.block.8.layer.2.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for decoder.block.9.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.9.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.9.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.9.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.9.layer.0.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for decoder.block.9.layer.1.EncDecAttention.q.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.9.layer.1.EncDecAttention.k.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.9.layer.1.EncDecAttention.v.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.9.layer.1.EncDecAttention.o.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.9.layer.1.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for decoder.block.9.layer.2.DenseReluDense.wi_0.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for decoder.block.9.layer.2.DenseReluDense.wi_1.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for decoder.block.9.layer.2.DenseReluDense.wo.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for decoder.block.9.layer.2.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for decoder.block.10.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.10.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.10.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.10.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.10.layer.0.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for decoder.block.10.layer.1.EncDecAttention.q.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.10.layer.1.EncDecAttention.k.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.10.layer.1.EncDecAttention.v.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.10.layer.1.EncDecAttention.o.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.10.layer.1.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for decoder.block.10.layer.2.DenseReluDense.wi_0.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for decoder.block.10.layer.2.DenseReluDense.wi_1.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for decoder.block.10.layer.2.DenseReluDense.wo.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for decoder.block.10.layer.2.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for decoder.block.11.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.11.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.11.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.11.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.11.layer.0.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for decoder.block.11.layer.1.EncDecAttention.q.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.11.layer.1.EncDecAttention.k.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.11.layer.1.EncDecAttention.v.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.11.layer.1.EncDecAttention.o.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.11.layer.1.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for decoder.block.11.layer.2.DenseReluDense.wi_0.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for decoder.block.11.layer.2.DenseReluDense.wi_1.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for decoder.block.11.layer.2.DenseReluDense.wo.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for decoder.block.11.layer.2.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for decoder.final_layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Carregue o modelo com pesos do TensorFlow\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForSeq2SeqLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabspath\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels/flan-t5-base\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 🔥 Parâmetro obrigatório\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use float32 para CPU\u001b[39;49;00m\n\u001b[0;32m     10\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/flan-t5-base\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModelo e tokenizador carregados com sucesso!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:571\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mconfig_class \u001b[38;5;241m==\u001b[39m config\u001b[38;5;241m.\u001b[39msub_configs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    570\u001b[0m         config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_text_config()\n\u001b[1;32m--> 571\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    572\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    573\u001b[0m     )\n\u001b[0;32m    574\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    575\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    576\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    577\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages\\transformers\\modeling_utils.py:279\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    281\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[1;32mc:\\Users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages\\transformers\\modeling_utils.py:4384\u001b[0m, in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   4368\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m   4369\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mregister_for_auto_class\u001b[39m(\u001b[38;5;28mcls\u001b[39m, auto_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoModel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   4370\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4371\u001b[0m \u001b[38;5;124;03m    Register this class with a given auto class. This should only be used for custom models as the ones in the\u001b[39;00m\n\u001b[0;32m   4372\u001b[0m \u001b[38;5;124;03m    library are already mapped with an auto class.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4382\u001b[0m \u001b[38;5;124;03m            The auto class to register this new model with.\u001b[39;00m\n\u001b[0;32m   4383\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4384\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(auto_class, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   4385\u001b[0m         auto_class \u001b[38;5;241m=\u001b[39m auto_class\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m   4387\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mauto_module\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages\\transformers\\modeling_utils.py:4971\u001b[0m, in \u001b[0;36m_load_from_tf\u001b[1;34m(cls, model, config, checkpoint_files)\u001b[0m\n\u001b[0;32m   4969\u001b[0m files_content \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mdefaultdict(\u001b[38;5;28mlist\u001b[39m)\n\u001b[0;32m   4970\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m weight_name, filename \u001b[38;5;129;01min\u001b[39;00m weight_map\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m-> 4971\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(weight_name) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m weight_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m device_map:\n\u001b[0;32m   4972\u001b[0m         weight_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(weight_name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m   4973\u001b[0m     files_content[filename]\u001b[38;5;241m.\u001b[39mappend(device_map[weight_name])\n",
      "File \u001b[1;32mc:\\Users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages\\transformers\\modeling_tf_pytorch_utils.py:535\u001b[0m, in \u001b[0;36mload_tf2_checkpoint_in_pytorch_model\u001b[1;34m(pt_model, tf_checkpoint_path, tf_inputs, allow_missing_keys, output_loading_info)\u001b[0m\n\u001b[0;32m    531\u001b[0m     tf_model(tf_inputs, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)  \u001b[38;5;66;03m# Make sure model is built\u001b[39;00m\n\u001b[0;32m    533\u001b[0m load_tf_weights(tf_model, tf_checkpoint_path)\n\u001b[1;32m--> 535\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_tf2_model_in_pytorch_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    536\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpt_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_missing_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_missing_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_loading_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_loading_info\u001b[49m\n\u001b[0;32m    537\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages\\transformers\\modeling_tf_pytorch_utils.py:544\u001b[0m, in \u001b[0;36mload_tf2_model_in_pytorch_model\u001b[1;34m(pt_model, tf_model, allow_missing_keys, output_loading_info)\u001b[0m\n\u001b[0;32m    541\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load TF 2.0 model in a pytorch model\"\"\"\u001b[39;00m\n\u001b[0;32m    542\u001b[0m weights \u001b[38;5;241m=\u001b[39m tf_model\u001b[38;5;241m.\u001b[39mweights\n\u001b[1;32m--> 544\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_tf2_weights_in_pytorch_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpt_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_missing_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_missing_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_loading_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_loading_info\u001b[49m\n\u001b[0;32m    546\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages\\transformers\\modeling_tf_pytorch_utils.py:562\u001b[0m, in \u001b[0;36mload_tf2_weights_in_pytorch_model\u001b[1;34m(pt_model, tf_weights, allow_missing_keys, output_loading_info)\u001b[0m\n\u001b[0;32m    559\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m    561\u001b[0m tf_state_dict \u001b[38;5;241m=\u001b[39m {tf_weight\u001b[38;5;241m.\u001b[39mname: tf_weight\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mfor\u001b[39;00m tf_weight \u001b[38;5;129;01min\u001b[39;00m tf_weights}\n\u001b[1;32m--> 562\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_tf2_state_dict_in_pytorch_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    563\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpt_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf_state_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_missing_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_missing_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_loading_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_loading_info\u001b[49m\n\u001b[0;32m    564\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages\\transformers\\modeling_tf_pytorch_utils.py:632\u001b[0m, in \u001b[0;36mload_tf2_state_dict_in_pytorch_model\u001b[1;34m(pt_model, tf_state_dict, allow_missing_keys, output_loading_info)\u001b[0m\n\u001b[0;32m    629\u001b[0m     loaded_pt_weights_data_ptr[pt_weight\u001b[38;5;241m.\u001b[39mdata_ptr()] \u001b[38;5;241m=\u001b[39m array\n\u001b[0;32m    630\u001b[0m     all_tf_weights\u001b[38;5;241m.\u001b[39mdiscard(pt_weight_name)\n\u001b[1;32m--> 632\u001b[0m missing_keys, unexpected_keys \u001b[38;5;241m=\u001b[39m \u001b[43mpt_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_pt_params_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    633\u001b[0m missing_keys \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m missing_keys_pt\n\u001b[0;32m    635\u001b[0m \u001b[38;5;66;03m# Some models may have keys that are not in the state by design, removing them before needlessly warning\u001b[39;00m\n\u001b[0;32m    636\u001b[0m \u001b[38;5;66;03m# the user.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\clr_c\\anaconda3\\envs\\efd-ia\\lib\\site-packages\\torch\\nn\\modules\\module.py:2593\u001b[0m, in \u001b[0;36mload_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for T5ForConditionalGeneration:\n\tsize mismatch for encoder.block.0.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.0.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.0.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.0.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([32, 12]).\n\tsize mismatch for encoder.block.0.layer.0.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.block.0.layer.1.DenseReluDense.wi_0.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for encoder.block.0.layer.1.DenseReluDense.wi_1.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for encoder.block.0.layer.1.DenseReluDense.wo.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for encoder.block.0.layer.1.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.block.1.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.1.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.1.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.1.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.1.layer.0.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.block.1.layer.1.DenseReluDense.wi_0.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for encoder.block.1.layer.1.DenseReluDense.wi_1.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for encoder.block.1.layer.1.DenseReluDense.wo.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for encoder.block.1.layer.1.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.block.2.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.2.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.2.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.2.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.2.layer.0.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.block.2.layer.1.DenseReluDense.wi_0.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for encoder.block.2.layer.1.DenseReluDense.wi_1.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for encoder.block.2.layer.1.DenseReluDense.wo.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for encoder.block.2.layer.1.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.block.3.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.3.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.3.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.3.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.3.layer.0.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.block.3.layer.1.DenseReluDense.wi_0.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for encoder.block.3.layer.1.DenseReluDense.wi_1.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for encoder.block.3.layer.1.DenseReluDense.wo.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for encoder.block.3.layer.1.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.block.4.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.4.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.4.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.4.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.4.layer.0.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.block.4.layer.1.DenseReluDense.wi_0.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for encoder.block.4.layer.1.DenseReluDense.wi_1.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for encoder.block.4.layer.1.DenseReluDense.wo.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for encoder.block.4.layer.1.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.block.5.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.5.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.5.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.5.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.5.layer.0.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.block.5.layer.1.DenseReluDense.wi_0.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for encoder.block.5.layer.1.DenseReluDense.wi_1.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for encoder.block.5.layer.1.DenseReluDense.wo.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for encoder.block.5.layer.1.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.block.6.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.6.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.6.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.6.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.6.layer.0.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.block.6.layer.1.DenseReluDense.wi_0.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for encoder.block.6.layer.1.DenseReluDense.wi_1.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for encoder.block.6.layer.1.DenseReluDense.wo.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for encoder.block.6.layer.1.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.block.7.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.7.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.7.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.7.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.7.layer.0.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.block.7.layer.1.DenseReluDense.wi_0.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for encoder.block.7.layer.1.DenseReluDense.wi_1.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for encoder.block.7.layer.1.DenseReluDense.wo.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for encoder.block.7.layer.1.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.block.8.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.8.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.8.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.8.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.8.layer.0.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.block.8.layer.1.DenseReluDense.wi_0.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for encoder.block.8.layer.1.DenseReluDense.wi_1.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for encoder.block.8.layer.1.DenseReluDense.wo.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for encoder.block.8.layer.1.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.block.9.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.9.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.9.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.9.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.9.layer.0.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.block.9.layer.1.DenseReluDense.wi_0.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for encoder.block.9.layer.1.DenseReluDense.wi_1.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for encoder.block.9.layer.1.DenseReluDense.wo.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for encoder.block.9.layer.1.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.block.10.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.10.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.10.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.10.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.10.layer.0.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.block.10.layer.1.DenseReluDense.wi_0.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for encoder.block.10.layer.1.DenseReluDense.wi_1.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for encoder.block.10.layer.1.DenseReluDense.wo.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for encoder.block.10.layer.1.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.block.11.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.11.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.11.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.11.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.block.11.layer.0.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.block.11.layer.1.DenseReluDense.wi_0.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for encoder.block.11.layer.1.DenseReluDense.wi_1.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for encoder.block.11.layer.1.DenseReluDense.wo.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for encoder.block.11.layer.1.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.final_layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for decoder.block.0.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.0.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.0.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.0.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([32, 12]).\n\tsize mismatch for decoder.block.0.layer.0.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for decoder.block.0.layer.1.EncDecAttention.q.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.0.layer.1.EncDecAttention.k.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.0.layer.1.EncDecAttention.v.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.0.layer.1.EncDecAttention.o.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.0.layer.1.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for decoder.block.0.layer.2.DenseReluDense.wi_0.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for decoder.block.0.layer.2.DenseReluDense.wi_1.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for decoder.block.0.layer.2.DenseReluDense.wo.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for decoder.block.0.layer.2.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for decoder.block.1.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.1.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.1.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.1.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.1.layer.0.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for decoder.block.1.layer.1.EncDecAttention.q.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.1.layer.1.EncDecAttention.k.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.1.layer.1.EncDecAttention.v.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.1.layer.1.EncDecAttention.o.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.1.layer.1.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for decoder.block.1.layer.2.DenseReluDense.wi_0.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for decoder.block.1.layer.2.DenseReluDense.wi_1.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for decoder.block.1.layer.2.DenseReluDense.wo.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for decoder.block.1.layer.2.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for decoder.block.2.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.2.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.2.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.2.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.2.layer.0.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for decoder.block.2.layer.1.EncDecAttention.q.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.2.layer.1.EncDecAttention.k.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.2.layer.1.EncDecAttention.v.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.2.layer.1.EncDecAttention.o.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.2.layer.1.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for decoder.block.2.layer.2.DenseReluDense.wi_0.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for decoder.block.2.layer.2.DenseReluDense.wi_1.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for decoder.block.2.layer.2.DenseReluDense.wo.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for decoder.block.2.layer.2.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for decoder.block.3.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.3.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.3.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.3.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.3.layer.0.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for decoder.block.3.layer.1.EncDecAttention.q.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.3.layer.1.EncDecAttention.k.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.3.layer.1.EncDecAttention.v.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.3.layer.1.EncDecAttention.o.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.3.layer.1.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for decoder.block.3.layer.2.DenseReluDense.wi_0.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for decoder.block.3.layer.2.DenseReluDense.wi_1.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for decoder.block.3.layer.2.DenseReluDense.wo.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for decoder.block.3.layer.2.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for decoder.block.4.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.4.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.4.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.4.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.4.layer.0.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for decoder.block.4.layer.1.EncDecAttention.q.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.4.layer.1.EncDecAttention.k.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.4.layer.1.EncDecAttention.v.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.4.layer.1.EncDecAttention.o.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.4.layer.1.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for decoder.block.4.layer.2.DenseReluDense.wi_0.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for decoder.block.4.layer.2.DenseReluDense.wi_1.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for decoder.block.4.layer.2.DenseReluDense.wo.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for decoder.block.4.layer.2.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for decoder.block.5.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.5.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.5.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.5.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.5.layer.0.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for decoder.block.5.layer.1.EncDecAttention.q.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.5.layer.1.EncDecAttention.k.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.5.layer.1.EncDecAttention.v.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.5.layer.1.EncDecAttention.o.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.5.layer.1.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for decoder.block.5.layer.2.DenseReluDense.wi_0.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for decoder.block.5.layer.2.DenseReluDense.wi_1.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for decoder.block.5.layer.2.DenseReluDense.wo.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for decoder.block.5.layer.2.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for decoder.block.6.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.6.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.6.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.6.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.6.layer.0.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for decoder.block.6.layer.1.EncDecAttention.q.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.6.layer.1.EncDecAttention.k.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.6.layer.1.EncDecAttention.v.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.6.layer.1.EncDecAttention.o.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.6.layer.1.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for decoder.block.6.layer.2.DenseReluDense.wi_0.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for decoder.block.6.layer.2.DenseReluDense.wi_1.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for decoder.block.6.layer.2.DenseReluDense.wo.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for decoder.block.6.layer.2.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for decoder.block.7.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.7.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.7.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.7.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.7.layer.0.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for decoder.block.7.layer.1.EncDecAttention.q.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.7.layer.1.EncDecAttention.k.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.7.layer.1.EncDecAttention.v.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.7.layer.1.EncDecAttention.o.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.7.layer.1.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for decoder.block.7.layer.2.DenseReluDense.wi_0.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for decoder.block.7.layer.2.DenseReluDense.wi_1.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for decoder.block.7.layer.2.DenseReluDense.wo.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for decoder.block.7.layer.2.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for decoder.block.8.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.8.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.8.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.8.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.8.layer.0.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for decoder.block.8.layer.1.EncDecAttention.q.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.8.layer.1.EncDecAttention.k.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.8.layer.1.EncDecAttention.v.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.8.layer.1.EncDecAttention.o.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.8.layer.1.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for decoder.block.8.layer.2.DenseReluDense.wi_0.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for decoder.block.8.layer.2.DenseReluDense.wi_1.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for decoder.block.8.layer.2.DenseReluDense.wo.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for decoder.block.8.layer.2.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for decoder.block.9.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.9.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.9.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.9.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.9.layer.0.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for decoder.block.9.layer.1.EncDecAttention.q.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.9.layer.1.EncDecAttention.k.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.9.layer.1.EncDecAttention.v.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.9.layer.1.EncDecAttention.o.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.9.layer.1.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for decoder.block.9.layer.2.DenseReluDense.wi_0.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for decoder.block.9.layer.2.DenseReluDense.wi_1.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for decoder.block.9.layer.2.DenseReluDense.wo.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for decoder.block.9.layer.2.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for decoder.block.10.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.10.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.10.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.10.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.10.layer.0.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for decoder.block.10.layer.1.EncDecAttention.q.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.10.layer.1.EncDecAttention.k.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.10.layer.1.EncDecAttention.v.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.10.layer.1.EncDecAttention.o.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.10.layer.1.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for decoder.block.10.layer.2.DenseReluDense.wi_0.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for decoder.block.10.layer.2.DenseReluDense.wi_1.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for decoder.block.10.layer.2.DenseReluDense.wo.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for decoder.block.10.layer.2.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for decoder.block.11.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.11.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.11.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.11.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.11.layer.0.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for decoder.block.11.layer.1.EncDecAttention.q.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.11.layer.1.EncDecAttention.k.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.11.layer.1.EncDecAttention.v.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.11.layer.1.EncDecAttention.o.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for decoder.block.11.layer.1.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for decoder.block.11.layer.2.DenseReluDense.wi_0.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for decoder.block.11.layer.2.DenseReluDense.wi_1.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for decoder.block.11.layer.2.DenseReluDense.wo.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for decoder.block.11.layer.2.layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for decoder.final_layer_norm.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([768])."
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Carregue o modelo com pesos do TensorFlow\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    os.path.abspath(\"models/flan-t5-base\"),\n",
    "    from_tf=True,  # 🔥 Parâmetro obrigatório\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float32  # Use float32 para CPU\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(os.path.abspath(\"models/flan-t5-base\"))\n",
    "print(\"Modelo e tokenizador carregados com sucesso!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "efd-ia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
